{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db5ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf7753",
   "metadata": {},
   "source": [
    "# Tensor Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b1ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([3, 2])\n",
      "Size: torch.Size([3, 2])\n",
      "Dim: 2\n",
      "Dtype: torch.int64\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors\n",
    "x = torch.tensor([1, 2, 3])                    # From list\n",
    "y = torch.tensor([[1, 2], [3, 4], [5,6]])      # 2D tensor\n",
    "z = torch.zeros(2, 3)                          # Zeros: shape (2, 3)\n",
    "a = torch.ones(2, 3)                           # Ones: shape (2, 3)\n",
    "b = torch.randn(2, 3)                          # Random normal\n",
    "c = torch.arange(0, 10, 2)                     # Like range(): 0,2,4,6,8\n",
    "d = torch.linspace(0, 1, 5)                    # 5 points from 0 to 1\n",
    "\n",
    "# Common attributes\n",
    "print(f\"Shape: {y.shape}\")                     # torch.Size([2, 2])\n",
    "print(f\"Size: {y.size()}\")                     # Same as shape\n",
    "print(f\"Dim: {y.dim()}\")                       # Number of dimensions (2)\n",
    "print(f\"Dtype: {y.dtype}\")                     # Data type\n",
    "print(f\"Device: {y.device}\")                   # CPU/GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0affa",
   "metadata": {},
   "source": [
    "# Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic math\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "add = x + y                                    # Element-wise addition\n",
    "sub = x - y                                    # Subtraction\n",
    "mul = x * y                                    # Multiplication\n",
    "div = x / y                                    # Division\n",
    "pow = x ** 2                                   # Power\n",
    "\n",
    "# Matrix operations\n",
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "matmul = torch.matmul(A, B)                    # Matrix multiplication\n",
    "matmul_alt = A @ B                             # Same, Python style\n",
    "dot = torch.dot(x, y)                          # Dot product\n",
    "\n",
    "# Reduction operations\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "sum_all = tensor.sum()                         # Sum all elements\n",
    "sum_dim0 = tensor.sum(dim=0)                   # Sum along dimension 0\n",
    "mean = tensor.mean()                           # Mean\n",
    "std = tensor.std()                             # Standard deviation\n",
    "max_val, max_idx = tensor.max(dim=1)           # Max values and indices\n",
    "min_val = tensor.min()                         # Min value\n",
    "\n",
    "# Reshaping (CRITICAL for deep learning)\n",
    "tensor = torch.arange(12)\n",
    "\n",
    "view = tensor.view(3, 4)                       # Reshape to 3x4\n",
    "reshape = tensor.reshape(3, 4)                 # More flexible reshape\n",
    "transpose = tensor.view(3, 4).t()              # Transpose\n",
    "permute = tensor.view(3, 4).permute(1, 0)      # General permutation\n",
    "squeeze = torch.randn(1, 3, 1, 4).squeeze()    # Remove size 1 dims\n",
    "unsqueeze = tensor.unsqueeze(0)                # Add dimension at pos 0\n",
    "\n",
    "# Concatenation and stacking\n",
    "x1 = torch.tensor([1, 2, 3])\n",
    "x2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "cat = torch.cat([x1, x2], dim=0)               # Concatenate\n",
    "stack = torch.stack([x1, x2], dim=0)           # Stack along new dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f4c7d",
   "metadata": {},
   "source": [
    "# Neural Network Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce29a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear (Fully Connected)\n",
    "linear = nn.Linear(in_features=10, out_features=5)\n",
    "x = torch.randn(32, 10)  # batch_size=32, features=10\n",
    "output = linear(x)       # shape: (32, 5)\n",
    "\n",
    "# Convolutional\n",
    "\n",
    "# batch_size, num_features/channels, seq_length(number of days in time series)\n",
    "x = torch.randn(32, 10, 64)  \n",
    "# kernel_size: moving window size\n",
    "# padding: number of zeros added to each side\n",
    "# in_channels: input features/channels\n",
    "# out_channels: output features/channels\n",
    "conv1d = nn.Conv1d(in_channels=10, out_channels=20, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "conv2d = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "x = torch.randn(32, 3, 64, 64)  # (batch, channels, height, width)\n",
    "output = conv2d(x)               # (32, 16, 64, 64)\n",
    "\n",
    "# Recurrent\n",
    "lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)\n",
    "x = torch.randn(32, 5, 10)  # (batch, seq_len, features)\n",
    "output, (hn, cn) = lstm(x)  # output: (32, 5, 20)\n",
    "\n",
    "# Normalization layers\n",
    "batch_norm = nn.BatchNorm1d(num_features=64)\n",
    "layer_norm = nn.LayerNorm(normalized_shape=64)\n",
    "instance_norm = nn.InstanceNorm1d(num_features=64)\n",
    "\n",
    "# Dropout (Regularization)\n",
    "dropout = nn.Dropout(p=0.5)  # 50% dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ecd55",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11232b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# Functional style (preferred)\n",
    "relu = F.relu(x)\n",
    "sigmoid = torch.sigmoid(x)\n",
    "tanh = torch.tanh(x)\n",
    "softmax = F.softmax(x, dim=0)\n",
    "leaky_relu = F.leaky_relu(x, negative_slope=0.01)\n",
    "\n",
    "# Module style\n",
    "relu_layer = nn.ReLU()\n",
    "sigmoid_layer = nn.Sigmoid()\n",
    "tanh_layer = nn.Tanh()\n",
    "softmax_layer = nn.Softmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c037a",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07592c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression losses\n",
    "y_pred = torch.tensor([0.5, 0.8, 1.2])\n",
    "y_true = torch.tensor([1.0, 1.0, 1.0])\n",
    "\n",
    "mse_loss = F.mse_loss(y_pred, y_true)          # Mean Squared Error\n",
    "l1_loss = F.l1_loss(y_pred, y_true)            # L1/MAE Loss\n",
    "smooth_l1 = F.smooth_l1_loss(y_pred, y_true)   # Huber Loss\n",
    "\n",
    "# Classification losses\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1], [0.5, 2.0, 0.3]])\n",
    "targets = torch.tensor([0, 1])  # Class indices\n",
    "\n",
    "ce_loss = F.cross_entropy(logits, targets)     # Cross Entropy (most common)\n",
    "bce_loss = F.binary_cross_entropy_with_logits(logits[:, 0], targets.float())\n",
    "\n",
    "# Custom loss\n",
    "def custom_loss(pred, target):\n",
    "    return ((pred - target) ** 2).mean() + 0.01 * pred.abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e9b773",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c562f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# Different optimizers\n",
    "adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "rmsprop = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(adam, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102d969",
   "metadata": {},
   "source": [
    "# Training Loop Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device='cpu'):\n",
    "    \"\"\"Standard training epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        # Move to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad() # reset the gradients back to zero for the coming update\n",
    "        output = model(data) # forward pass\n",
    "        loss = F.cross_entropy(output, target) # get the loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ee345",
   "metadata": {},
   "source": [
    "# Classic Model Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template 1: MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# Template 2: CNN \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Template 3: Transformer \n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, features)\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Pool over sequence\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a4803",
   "metadata": {},
   "source": [
    "# Dataloader & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Method 1: TensorDataset (quick and dirty)\n",
    "features = torch.randn(1000, 10)\n",
    "labels = torch.randint(0, 2, (1000,))\n",
    "dataset = TensorDataset(features, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Method 2: Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Method 3: Advanced Dataset (with transforms)\n",
    "class AdvancedDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data = np.load(data_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "\n",
    "# Common transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3b208",
   "metadata": {},
   "source": [
    "# Device Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bab348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move tensors/model to device\n",
    "x = torch.randn(10, 10).to(device)\n",
    "model = SimpleMLP(10, 20, 2).to(device)\n",
    "\n",
    "# Multi-GPU (if available)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Clear GPU memory (if needed)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5553a9e",
   "metadata": {},
   "source": [
    "# Model Saving & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda926ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleMLP(10, 20, 2)\n",
    "\n",
    "# Save entire model (easy but less flexible)\n",
    "torch.save(model, 'model.pth')\n",
    "loaded_model = torch.load('model.pth')\n",
    "\n",
    "# Save only state_dict (recommended)\n",
    "torch.save(model.state_dict(), 'model_state.pth')\n",
    "\n",
    "# Load state_dict\n",
    "new_model = SimpleMLP(10, 20, 2)\n",
    "new_model.load_state_dict(torch.load('model_state.pth'))\n",
    "\n",
    "# Save checkpoint (for resuming training)\n",
    "checkpoint = {\n",
    "    'epoch': 10,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': adam.state_dict(),\n",
    "    'loss': 0.05,\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "adam.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45775096",
   "metadata": {},
   "source": [
    "# Gradient & Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6071dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/disable gradient tracking\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Manual gradient computation\n",
    "y = x.sum()\n",
    "y.backward()  # Compute gradients\n",
    "print(f\"Gradients: {x.grad}\")\n",
    "\n",
    "# Disable gradient for inference\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "    # No gradient tracking here\n",
    "\n",
    "# Gradient accumulation (for large batches)\n",
    "for i, (data, target) in enumerate(dataloader):\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    \n",
    "    # Scale loss for accumulation\n",
    "    loss = loss / 4  # Accumulate 4 batches\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    if (i + 1) % 4 == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93056895",
   "metadata": {},
   "source": [
    "# Debugging & Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN/inf\n",
    "x = torch.tensor([1.0, float('nan'), 3.0])\n",
    "print(f\"Has NaN: {torch.isnan(x).any()}\")\n",
    "print(f\"Has Inf: {torch.isinf(x).any()}\")\n",
    "\n",
    "# Gradient checking\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} gradient mean: {param.grad.mean():.6f}\")\n",
    "        if torch.isnan(param.grad).any():\n",
    "            print(f\"⚠️ NaN gradient in {name}!\")\n",
    "\n",
    "# Model summary\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Memory usage\n",
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"GPU Memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd920a",
   "metadata": {},
   "source": [
    "# Useful Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# One-hot encoding\n",
    "labels = torch.tensor([0, 2, 1, 0])\n",
    "one_hot = F.one_hot(labels, num_classes=3)\n",
    "\n",
    "# Create masks\n",
    "sequence = torch.tensor([[1, 2, 0, 0], [1, 2, 3, 0]])\n",
    "mask = (sequence != 0)  # Padding mask\n",
    "\n",
    "# Top-k predictions\n",
    "logits = torch.randn(10, 5)  # 10 samples, 5 classes\n",
    "topk_values, topk_indices = torch.topk(logits, k=3, dim=1)\n",
    "\n",
    "# Batch matrix multiplication\n",
    "batch1 = torch.randn(32, 10, 20)  # (batch, n, m)\n",
    "batch2 = torch.randn(32, 20, 30)  # (batch, m, p)\n",
    "result = torch.bmm(batch1, batch2)  # (batch, n, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a19bf2",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduling\n",
    "def lr_scheduling():\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    for epoch in range(100):\n",
    "        train()\n",
    "        scheduler.step()\n",
    "\n",
    "# Gradient accumulation\n",
    "def grad_accumulation():\n",
    "    optimizer.zero_grad()\n",
    "    for i, (data, target) in enumerate(dataloader):\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target) / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "# Mixed precision training (faster on modern GPUs)\n",
    "def mixed_precision():\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
